"""
AI Response Generator for Mediflux V2
Generates intelligent, contextual responses using Grok API
Combines structured data with natural language generation
"""

import os
import asyncio
import json
from typing import Dict, List, Any, Optional
import aiohttp
import logging
from dotenv import load_dotenv


class AIResponseGenerator:
    """
    Generates intelligent responses using Grok API
    Combines orchestrator results with user context for natural responses
    """
    
    def __init__(self):
        # Load environment variables
        load_dotenv()
        self.logger = logging.getLogger(__name__)
        
        # Try to get Grok API key from environment
        self.api_key = os.getenv("XAI_API_KEY") or os.getenv("GROK_API_KEY")
        self.api_base = "https://api.x.ai/v1"
        
        # Fallback to OpenAI-compatible API if no Grok key
        if not self.api_key:
            self.api_key = os.getenv("OPENAI_API_KEY")
            self.api_base = "https://api.openai.com/v1"
        
        self.model = "grok-2" if "x.ai" in self.api_base else "gpt-3.5-turbo"
        
    async def generate_response(
        self, 
        user_query: str, 
        intent: str, 
        orchestrator_results: Dict[str, Any], 
        user_context: Dict[str, Any]
    ) -> str:
        """
        Generate intelligent response based on query, intent, and results
        
        Args:
            user_query: Original user question
            intent: Detected intent from orchestrator
            orchestrator_results: Structured data from orchestrator
            user_context: User profile and history
            
        Returns:
            Natural language response
        """
        try:
            # If no API key, return enhanced fallback
            if not self.api_key:
                return self._generate_fallback_response(user_query, intent, orchestrator_results, user_context)
            
            # Build context-aware prompt
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(user_query, intent, orchestrator_results, user_context)
            
            # Call LLM API
            response = await self._call_llm_api(system_prompt, user_prompt)
            
            return response
            
        except Exception as e:
            self.logger.error(f"AI response generation failed: {str(e)}")
            return self._generate_fallback_response(user_query, intent, orchestrator_results, user_context)
    
    def _build_system_prompt(self) -> str:
        """Build system prompt for French healthcare AI assistant"""
        return """Tu es un assistant IA expert du systÃ¨me de santÃ© franÃ§ais. 

ğŸ¯ OBJECTIF : Fournir des rÃ©ponses DIRECTES, UTILES et ACTIONABLES.

ğŸ“‹ TES CAPACITÃ‰S :
- Base BDPM (mÃ©dicaments officiels + prix + remboursements)
- Annuaire SantÃ© CNAM (praticiens + secteurs + spÃ©cialitÃ©s)
- Simulation remboursements (SÃ©cu + mutuelles)
- Analyse documents mÃ©dicaux
- Parcours de soins optimisÃ©s

âœ… STYLE DE RÃ‰PONSE REQUIS :
- DIRECTE : RÃ©ponds immÃ©diatement Ã  la question
- CONCRÃˆTE : Donne des informations prÃ©cises et actionables
- STRUCTURÃ‰E : Utilise des emojis (ğŸ’Š mÃ©dicaments, ğŸ’° coÃ»ts, ğŸ¥ praticiens)
- FRANÃ‡AISE : Exclusivement en franÃ§ais
- SOURCES : Mentionne les bases de donnÃ©es utilisÃ©es
- ACTIONS : Propose des Ã©tapes concrÃ¨tes

âŒ Ã‰VITE ABSOLUMENT :
- Les introductions longues ("Tout d'abord, analysons...")
- Les explications de ton processus de rÃ©flexion
- Les rÃ©pÃ©titions de la question utilisateur
- Les rÃ©ponses vagues ou gÃ©nÃ©rales

ï¿½ EXEMPLE DE BONNE RÃ‰PONSE :
User: "Prix du Doliprane ?"
Toi: "ğŸ’Š DOLIPRANE 1000mg : 2,50â‚¬ (BDPM)
ğŸ’° Remboursement : 15% SÃ©cu + mutuelle
ğŸ¯ Action : PrÃ©senter ordonnance en pharmacie"

RÃ©ponds TOUJOURS de maniÃ¨re directe et utile."""

    def _build_user_prompt(
        self, 
        user_query: str, 
        intent: str, 
        results: Dict[str, Any], 
        user_context: Dict[str, Any]
    ) -> str:
        """Build user-specific prompt with context and results"""
        
        prompt = f"QUESTION DE L'UTILISATEUR : {user_query}\n\n"
        prompt += f"INTENT DÃ‰TECTÃ‰ : {intent}\n\n"
        
        # Add user context if available
        profile = user_context.get("profile", {})
        if profile:
            prompt += "PROFIL UTILISATEUR :\n"
            if "mutuelle_type" in profile:
                prompt += f"- Mutuelle : {profile['mutuelle_type']}\n"
            if "pathology" in profile:
                prompt += f"- Pathologie : {profile['pathology']}\n"
            if "preferences" in profile:
                prompt += f"- PrÃ©fÃ©rences : {profile['preferences']}\n"
            prompt += "\n"
        
        # Add orchestrator results
        if results and results.get("success", True):
            # For care pathway with structured data, provide minimal context to avoid duplication
            if intent == "care_pathway" and results.get("type") == "care_pathway" and results.get("pathway", {}).get("evidence"):
                prompt += "RÃ‰SULTATS DU SYSTÃˆME :\n"
                pathway = results.get("pathway", {})
                prompt += f"- Condition: {pathway.get('condition', 'N/A')}\n"
                prompt += f"- Evidence: {pathway.get('evidence', {}).get('level', 'N/A')} ({pathway.get('evidence', {}).get('source', 'N/A')})\n"
                prompt += f"- Structured data available: pathway steps, medications, quality indicators\n\n"
            else:
                prompt += "RÃ‰SULTATS DU SYSTÃˆME :\n"
                prompt += json.dumps(results, indent=2, ensure_ascii=False)
                prompt += "\n\n"
        
        prompt += """CONSIGNES DE RÃ‰PONSE :
1. RÃ‰PONDS DIRECTEMENT Ã  la question sans prÃ©ambule
2. Pour les parcours de soins avec donnÃ©es structurÃ©es: DONNE SEULEMENT UN TITRE COURT ET REDIRIGE vers les dÃ©tails visuels
3. Ã‰VITE ABSOLUMENT de mentionner des Ã©tapes spÃ©cifiques, mÃ©dicaments, ou coÃ»ts qui seront affichÃ©s visuellement
4. FORMAT: Titre + brÃ¨ve mention de fiabilitÃ© + direction vers les dÃ©tails structurÃ©s
5. MAXIMUM 2-3 lignes de texte pour les parcours avec donnÃ©es structurÃ©es
6. MENTIONNE la source d'Ã©vidence si pertinente
7. PAS de questions de suivi pour les parcours structurÃ©s

EXEMPLE POUR PARCOURS STRUCTURÃ‰:
"ğŸ“‹ **Parcours personnalisÃ©** (Niveau A, 95% fiabilitÃ©)

Consultez les recommandations dÃ©taillÃ©es ci-dessous."

RÃ‰PONSE CONCISE :"""
        
        return prompt
    
    async def _call_llm_api(self, system_prompt: str, user_prompt: str) -> str:
        """Call LLM API (Grok or OpenAI) with rate limiting"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "max_tokens": 500,
                "temperature": 0.7
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.api_base}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        choice = data["choices"][0]["message"]
                        # grok-4-0709 provides direct content (no reasoning_content needed)
                        content = choice.get("content", "").strip()
                        return content
                    elif response.status == 429:
                        # Rate limited - wait and retry once
                        self.logger.warning("Rate limited, waiting 5 seconds...")
                        await asyncio.sleep(5)
                        
                        # Retry once
                        async with session.post(
                            f"{self.api_base}/chat/completions",
                            headers=headers,
                            json=payload,
                            timeout=aiohttp.ClientTimeout(total=30)
                        ) as retry_response:
                            if retry_response.status == 200:
                                retry_data = await retry_response.json()
                                retry_choice = retry_data["choices"][0]["message"]
                                retry_content = retry_choice.get("content", "").strip()
                                return retry_content
                            else:
                                retry_error = await retry_response.text()
                                raise Exception(f"Retry failed {retry_response.status}: {retry_error}")
                    else:
                        error_text = await response.text()
                        raise Exception(f"API error {response.status}: {error_text}")
                        
        except Exception as e:
            raise Exception(f"LLM API call failed: {str(e)}")
    
    def _generate_fallback_response(
        self, 
        user_query: str, 
        intent: str, 
        results: Dict[str, Any], 
        user_context: Dict[str, Any]
    ) -> str:
        """Generate intelligent fallback when LLM is not available"""
        
        profile = user_context.get("profile", {})
        
        if intent == "medication_info":
            medication_data = results.get("medication_data", {})
            if medication_data.get("success"):
                meds = medication_data.get("results", [])
                if meds:
                    med = meds[0]
                    return f"ğŸ’Š **{med.get('denomination', 'MÃ©dicament')}**\n\n" \
                           f"ğŸ’° Prix public : {med.get('public_price', 'N/A')}â‚¬\n" \
                           f"ğŸ“‹ Statut : {med.get('commercialization_status', 'N/A')}\n\n" \
                           f"*DonnÃ©es BDPM officielles*\n\n" \
                           f"Souhaitez-vous simuler le remboursement ?"
            return f"ğŸ’Š Je recherche des informations sur ce mÃ©dicament...\n\n" \
                   f"Puis-je vous aider avec autre chose ?"
        
        elif intent == "practitioner_search":
            search_results = results.get("search_results", {})
            if search_results.get("success"):
                total = search_results.get("total_found", 0)
                specialty = search_results.get("specialty", "praticiens")
                location = search_results.get("location", "")
                
                response = f"ğŸ‘©â€âš•ï¸ **Recherche de {specialty}**\n\n"
                if total > 0:
                    response += f"âœ… {total} {specialty}s trouvÃ©s"
                    if location:
                        response += f" (recherche nationale)"
                    response += f"\n\nğŸ“ *Filtrage par ville non disponible dans l'API Annuaire SantÃ©*"
                else:
                    response += f"âŒ Aucun {specialty} trouvÃ©"
                
                response += f"\n\nğŸ’¡ Astuce : PrÃ©fÃ©rez les praticiens secteur 1 pour minimiser les frais"
                return response
            
        elif intent == "care_pathway":
            pathway_data = results.get("pathway", {})
            if pathway_data.get("success"):
                condition = pathway_data.get("condition", "votre pathologie")
                evidence_level = pathway_data.get("evidence", {}).get("level", "")
                confidence = pathway_data.get("evidence", {}).get("confidence", 0)
                
                # Check if we have comprehensive structured data
                has_rich_data = (
                    pathway_data.get("evidence") and 
                    pathway_data.get("medications") and 
                    len(pathway_data.get("medications", [])) > 0 and
                    pathway_data.get("pathway_steps") and 
                    len(pathway_data.get("pathway_steps", [])) > 0
                )
                
                if has_rich_data:
                    # Minimal, non-redundant response for rich structured data
                    confidence_text = f"{int(confidence*100)}% fiabilitÃ©" if confidence > 0 else "haute fiabilitÃ©"
                    return f"ğŸ“‹ **Parcours personnalisÃ©** (Niveau {evidence_level}, {confidence_text})\n\nConsultez les recommandations dÃ©taillÃ©es ci-dessous."
                else:
                    # Detailed fallback when no structured data available
                    return f"ğŸ—ºï¸ **Parcours de soins optimisÃ©**\n\n" \
                           f"ğŸ“‹ Recommandations Ã©tablies selon votre profil\n" \
                           f"ğŸ’° Estimation des coÃ»ts incluse\n\n" \
                           f"Souhaitez-vous plus de dÃ©tails sur une Ã©tape ?"
        
        elif intent == "simulate_cost":
            simulation = results.get("simulation", {})
            if simulation.get("success"):
                return f"ğŸ’° **Simulation de remboursement**\n\n" \
                       f"ğŸ“Š Calculs effectuÃ©s" + (f" pour votre mutuelle {profile.get('mutuelle_type', '')}" if profile else "") + \
                       f"\n\nğŸ’¡ Les montants dÃ©pendent de votre situation exacte"
        
        # General fallback
        user_name = profile.get("name", "")
        greeting = f"Bonjour{' ' + user_name if user_name else ''} ! "
        
        return f"{greeting}Je comprends que vous vous renseignez sur : **{user_query}**\n\n" \
               f"ğŸ”§ *SystÃ¨me en cours de traitement...*\n\n" \
               f"ğŸ’¡ En attendant, je peux vous aider avec :\n" \
               f"â€¢ ğŸ’Š Informations sur les mÃ©dicaments\n" \
               f"â€¢ ğŸ’° Simulations de remboursement\n" \
               f"â€¢ ğŸ¥ Recherche de praticiens\n" \
               f"â€¢ ğŸ“„ Analyse de documents mÃ©dicaux\n\n" \
               f"Que souhaitez-vous faire ?"
